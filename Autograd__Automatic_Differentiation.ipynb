{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Autograd:_Automatic_Differentiation.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyO+jIz6nvR+ri4I5Qh1nOQh",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dixiong777/DL_Pytorch/blob/master/Autograd__Automatic_Differentiation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pWKV73e5Iz4e"
      },
      "source": [
        "### Autograd: Automatic Differentiation\n",
        "\n",
        "---\n",
        "\n",
        "Key python package for the neural network on GPU is `autograd`. This notebook will briefly go over it. \n",
        "\n",
        "More details on [Here](https://pytorch.org/tutorials/beginner/blitz/autograd_tutorial.html#sphx-glr-beginner-blitz-autograd-tutorial-py)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A9A7jj9HKcPY"
      },
      "source": [
        "### 1. Tensor and Functions\n",
        "\n",
        "* `.detach()`: stop a tensor from tracking history and prevent future computation from being tracked.\n",
        "\n",
        "* `.requires_grad = True` and `.backward()`: track all operations and have all gradients computed automatically. The gradient for this tensor will be accumulated into `.grad` attribute.\n",
        "\n",
        "  **Note**: If the tensor is a scalar, we don't need to specify any arguments to `.backward()`, however, if there is more than one element, we need to specify the `gradient` argument matching the shape of the tensor.\n",
        "\n",
        "\n",
        "* `.grad_fn`: Each tensor has a `.grad_fn` attribute that references a `Function` that created this tensor (**except** for tensor created by the user directly.)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h3727zVCIsnK"
      },
      "source": [
        "import torch"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z_sufxSgJh5O",
        "outputId": "76875605-80a1-417b-b8eb-178f1a342d87"
      },
      "source": [
        "# Create a tensor and set requires_grad = True\n",
        "x = torch.ones(2, 2, requires_grad = True)\n",
        "print(x)\n",
        "print(x.grad_fn)"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[1., 1.],\n",
            "        [1., 1.]], requires_grad=True)\n",
            "None\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wkGviYa7KYE5",
        "outputId": "2c1dd5dd-c33f-4957-f204-97de37c53267"
      },
      "source": [
        "# Do an operation\n",
        "y = x + 2\n",
        "print(y)\n",
        "print(y.grad_fn)"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[3., 3.],\n",
            "        [3., 3.]], grad_fn=<AddBackward0>)\n",
            "<AddBackward0 object at 0x7f978f61c0b8>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-2aufveAMeJa",
        "outputId": "f8c1adb8-e5c9-4fe5-c49b-bc67ba8f040a"
      },
      "source": [
        "# Do more operations\n",
        "z = y * y * 3\n",
        "out = z.mean()\n",
        "print(z, '\\n', out)"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[27., 27.],\n",
            "        [27., 27.]], grad_fn=<MulBackward0>) \n",
            " tensor(27., grad_fn=<MeanBackward0>)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-71Iou9nMpaf",
        "outputId": "039df996-d7b9-4a49-c4d9-f383d472d244"
      },
      "source": [
        "# `.requires_grad()` default as False\n",
        "# Cannot get the grad function \n",
        "a = torch.randn(2, 2)\n",
        "print(a, a.requires_grad, a.grad_fn)\n",
        "\n",
        "a = ((a * 3) / (a - 1))\n",
        "print(a, a.requires_grad, a.grad_fn)\n",
        "\n",
        "b = (a * a).sum()\n",
        "print(b.grad_fn)\n",
        "\n",
        "# After update, we can get the grad function.\n",
        "a.requires_grad_(True)\n",
        "print(a.requires_grad)\n",
        "\n",
        "b = (a * a).sum()\n",
        "print(b.grad_fn)"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[-0.1634, -0.6950],\n",
            "        [ 0.8751,  1.1640]]) False None\n",
            "tensor([[  0.4213,   1.2301],\n",
            "        [-21.0248,  21.2918]]) False None\n",
            "None\n",
            "True\n",
            "<SumBackward0 object at 0x7f978f608198>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rvrbERul39Bn"
      },
      "source": [
        "### 2. Gradients and Vector-Jacobian Product\n",
        "Main component for the Neural Networks.\n",
        "\n",
        "* Have the `.backward()` first and then calculate the grad.\n",
        "\n",
        "  **Note**: Only can call `.backward()` once and calcualte the grad on the the leaf tensor when `.requires_grad = True` using `.grad` method. Otherwise, use `.retain_grad` for non-leaf tensor.\n",
        "\n",
        "* `torch.autograd` is an engine for computing\n",
        "vector-Jacobian product. That is, given any vector\n",
        "$v=\\left(\\begin{array}{cccc} v_{1} & v_{2} & \\cdots & v_{m}\\end{array}\\right)^{T}$,\n",
        "compute the product $v^{T}\\cdot J$. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EY85fnUX4NvD",
        "outputId": "317ebd46-f5cb-4883-8f9b-5be68f210b86"
      },
      "source": [
        "# Backward.\n",
        "# Have the requires_grad = True first.\n",
        "x = torch.ones(2, 2, requires_grad = True)\n",
        "y = x + 2\n",
        "z = y * y * 3\n",
        "out = z.mean()\n",
        "print(out)"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor(27., grad_fn=<MeanBackward0>)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8rCXd-N45VWd"
      },
      "source": [
        "For the tensor `out`. We have $out = \\frac{1}{4}\\sum_i z_i$ where $z_i = 3 * (x_i + 2)^2$. Therefore for all $i = 1, 2, 3, 4$,\n",
        "$$\\frac{\\partial out}{\\partial x_i}|_{x_i = 1} = \\frac{3}{2} * (x_i + 2) = 4.5$$\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ppf9-Xfr4QSs",
        "outputId": "651ff447-4b67-4b2a-befe-faea141f7bdc"
      },
      "source": [
        "# Jacobian Matrix: Gradients d(out) / dx\n",
        "out = z.mean()\n",
        "out.backward()\n",
        "print(x.grad)\n",
        "print(y.retain_grad)"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[4.5000, 4.5000],\n",
            "        [4.5000, 4.5000]])\n",
            "<bound method Tensor.retain_grad of tensor([[3., 3.],\n",
            "        [3., 3.]], grad_fn=<AddBackward0>)>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v_WwCxHcASSF"
      },
      "source": [
        "Suppose $\\vec{y}=f(\\vec{x})$ with the gradient of $\\vec{y}$ with respect to $\\vec{x}$\n",
        "as a Jacobian matrix:\n",
        "\n",
        "\\begin{align}J=\\left(\\begin{array}{ccc}\n",
        "   \\frac{\\partial y_{1}}{\\partial x_{1}} & \\cdots & \\frac{\\partial y_{1}}{\\partial x_{n}}\\\\\n",
        "   \\vdots & \\ddots & \\vdots\\\\\n",
        "   \\frac{\\partial y_{m}}{\\partial x_{1}} & \\cdots & \\frac{\\partial y_{m}}{\\partial x_{n}}\n",
        "   \\end{array}\\right)\\end{align}\n",
        "\n",
        "\n",
        "Assign $v=\\left(\\begin{array}{ccc}\\frac{\\partial l}{\\partial y_{1}} & \\cdots & \\frac{\\partial l}{\\partial y_{m}}\\end{array}\\right)^{T}$ where, $l=g\\left(\\vec{y}\\right)$. \n",
        "Based on **the chain rule**, the vector-Jacobian product would be the\n",
        "gradient of $l$ with respect to $\\vec{x}$:\n",
        "\n",
        "\\begin{align}J^{T}\\cdot v=\\left(\\begin{array}{ccc}\n",
        "   \\frac{\\partial y_{1}}{\\partial x_{1}} & \\cdots & \\frac{\\partial y_{m}}{\\partial x_{1}}\\\\\n",
        "   \\vdots & \\ddots & \\vdots\\\\\n",
        "   \\frac{\\partial y_{1}}{\\partial x_{n}} & \\cdots & \\frac{\\partial y_{m}}{\\partial x_{n}}\n",
        "   \\end{array}\\right)\\left(\\begin{array}{c}\n",
        "   \\frac{\\partial l}{\\partial y_{1}}\\\\\n",
        "   \\vdots\\\\\n",
        "   \\frac{\\partial l}{\\partial y_{m}}\n",
        "   \\end{array}\\right)=\\left(\\begin{array}{c}\n",
        "   \\frac{\\partial l}{\\partial x_{1}}\\\\\n",
        "   \\vdots\\\\\n",
        "   \\frac{\\partial l}{\\partial x_{n}}\n",
        "   \\end{array}\\right)\\end{align}"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_ROeH3jR5Jie",
        "outputId": "64efd6e1-7402-45c7-9cf0-2416adc1992f"
      },
      "source": [
        "# An example of vector-Jacobian product:\n",
        "x = torch.randn(3, requires_grad = True)\n",
        "y = x * 2\n",
        "while y.data.norm() < 1000:\n",
        "  y = y * 2\n",
        "\n",
        "print(y)\n",
        "\n",
        "# Sine y is a vector, we only can get the vector-jacobian product\n",
        "v = torch.tensor([0.1, 1.0, 0.0001], dtype = torch.float)\n",
        "y.backward(v)\n",
        "print(x.grad)"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([-1408.8143,   136.5251,   675.6753], grad_fn=<MulBackward0>)\n",
            "tensor([1.0240e+02, 1.0240e+03, 1.0240e-01])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SKrDHXY_HEJL"
      },
      "source": [
        "Stop autograd from tracking history on Tensors with .requires_grad = True by \n",
        "\n",
        "* The code block `with torch.no_grad()`.\n",
        "  \n",
        "  **Note**: It only works for the no-leaf variables. \n",
        "\n",
        "* Using `.detach()` to get a new Tensor with the same content."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e9BOfzvpC23a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e6ca5024-ae74-4b32-aeb9-89870f7d552e"
      },
      "source": [
        "# with toch.no_rad block.\n",
        "print(x.requires_grad)\n",
        "print((x ** 2).requires_grad)\n",
        "with torch.no_grad():\n",
        "  print(x.requires_grad)\n",
        "  print((x ** 2).requires_grad)"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "True\n",
            "True\n",
            "True\n",
            "False\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UaAesYkjHAS7",
        "outputId": "93efe004-4e5e-4103-cce0-f547ebaa3ca7"
      },
      "source": [
        "# .detach()\n",
        "# After detached, it is as requires.grad = False by default.\n",
        "print(x.requires_grad)\n",
        "y = x.detach()\n",
        "print(y.requires_grad)\n",
        "\n",
        "# Test whether the content is still the same.\n",
        "print(x.eq(y).all())"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "True\n",
            "False\n",
            "tensor(True)\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}